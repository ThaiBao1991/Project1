import os
import json
import sys
import re
from urllib.parse import urlparse
from datetime import datetime
import requests
from bs4 import BeautifulSoup

class TruyenDownloader:
    def __init__(self, config_file='config.json'):
        self.config_file = config_file
        self.load_config()
        
    def load_config(self):
        """Load c·∫•u h√¨nh t·ª´ file"""
        if os.path.exists(self.config_file):
            with open(self.config_file, 'r', encoding='utf-8') as f:
                self.config = json.load(f)
        else:
            # T·∫°o c·∫•u h√¨nh m·∫∑c ƒë·ªãnh
            self.config = {
                "storage_path": "storage",
                "download_path": "downloads",
                "default_site": "tangthuvien",
                "browser": {
                    "headless": False,
                    "user_data_dir": "storage/browser_data"
                }
            }
            self.save_config()
        
        # G√°n c√°c ƒë∆∞·ªùng d·∫´n
        self.storage_path = self.config['storage_path']
        self.download_path = self.config['download_path']
        
        # T·∫°o th∆∞ m·ª•c c·∫ßn thi·∫øt
        os.makedirs(self.storage_path, exist_ok=True)
        os.makedirs(self.download_path, exist_ok=True)
        os.makedirs(os.path.join(self.storage_path, 'cookies'), exist_ok=True)
        
        # Load sites config
        self.sites_config = self.load_sites_config()
    
    def save_config(self):
        """L∆∞u c·∫•u h√¨nh"""
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(self.config, f, ensure_ascii=False, indent=2)
    
    def load_sites_config(self):
        """Load c·∫•u h√¨nh c√°c sites t·ª´ file"""
        sites_file = os.path.join(self.storage_path, 'sites_config.json')
        if os.path.exists(sites_file):
            with open(sites_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            # T·∫°o c·∫•u h√¨nh m·∫∑c ƒë·ªãnh cho Tangthuvien
            default_config = {
                "tangthuvien": {
                    "domain": "tangthuvien.net",
                    "selectors": {
                        "title": {
                            "type": "css",
                            "selector": ".chapter-title, h1.chapter-title, .chap-title"
                        },
                        "content": {
                            "type": "css", 
                            "selector": ".chapter-content, .content, #chapter-content"
                        },
                        "toc": {
                            "type": "css",
                            "selector": ".chapter-list a, .list-chapter a, .toc a"
                        }
                    },
                    "toc_page": "https://tangthuvien.net/doc-truyen/{story_slug}/",
                    "chapter_url_pattern": "/doc-truyen/{story_slug}/chuong-{chapter_number}",
                    "needs_login": True
                }
            }
            self.save_sites_config(default_config)
            return default_config
    
    def save_sites_config(self, config):
        """L∆∞u c·∫•u h√¨nh sites"""
        sites_file = os.path.join(self.storage_path, 'sites_config.json')
        with open(sites_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
    
    def get_site_parser(self, url):
        """X√°c ƒë·ªãnh parser d·ª±a tr√™n URL"""
        parsed = urlparse(url)
        domain = parsed.netloc.replace('www.', '')
        
        for site_name, site_config in self.sites_config.items():
            if site_config['domain'] in domain:
                if site_name == 'tangthuvien':
                    return TangThuVienParser(site_config, self)
        
        return None
    
    def extract_story_info(self, url):
        """Tr√≠ch xu·∫•t th√¥ng tin truy·ªán t·ª´ URL"""
        parsed = urlparse(url)
        path_parts = parsed.path.split('/')
        
        # Format: /doc-truyen/ten-truyen/chuong-231
        if 'doc-truyen' in path_parts:
            idx = path_parts.index('doc-truyen')
            if idx + 1 < len(path_parts):
                story_slug = path_parts[idx + 1]
                
                # X√°c ƒë·ªãnh chapter n·∫øu c√≥
                chapter = None
                for part in path_parts:
                    if 'chuong-' in part:
                        chapter = part
                        break
                
                # T·∫°o t√™n truy·ªán t·ª´ slug
                story_name = ' '.join(word.capitalize() for word in story_slug.split('-'))
                
                return {
                    'story_slug': story_slug,
                    'story_name': story_name,
                    'chapter': chapter,
                    'base_url': f"{parsed.scheme}://{parsed.netloc}",
                    'url': url
                }
        return None
    
    def login_to_site(self, site_name, url):
        """ƒêƒÉng nh·∫≠p v√†o site s·ª≠ d·ª•ng Chromium"""
        print(f"\n=== ƒêƒÇNG NH·∫¨P V√ÄO {site_name.upper()} ===")
        print("Vui l√≤ng ƒëƒÉng nh·∫≠p v√†o t√†i kho·∫£n c·ªßa b·∫°n trong tr√¨nh duy·ªát")
        print("Sau khi ƒëƒÉng nh·∫≠p xong, ƒë√≥ng tr√¨nh duy·ªát ƒë·ªÉ ti·∫øp t·ª•c...")
        
        cookie_file = os.path.join(self.storage_path, 'cookies', f'{site_name}.json')
        
        try:
            from utils.browser import BrowserManager
            with BrowserManager(headless=False) as browser:
                browser.navigate(url)
                input("Nh·∫•n Enter sau khi ƒë√£ ƒëƒÉng nh·∫≠p xong v√† ƒë√≥ng tr√¨nh duy·ªát...")
                
                # L∆∞u cookies
                cookies = browser.get_cookies()
                with open(cookie_file, 'w', encoding='utf-8') as f:
                    json.dump(cookies, f)
                
                print("ƒê√£ l∆∞u th√¥ng tin ƒëƒÉng nh·∫≠p!")
                return cookies
        except ImportError:
            print("Kh√¥ng th·ªÉ import BrowserManager. Vui l√≤ng c√†i ƒë·∫∑t selenium.")
            return None
    
    def download_chapter(self, url, parser, use_login=False):
        """T·∫£i m·ªôt chapter"""
        print(f"ƒêang t·∫£i: {url}")
        
        browser = None
        try:
            if use_login:
                try:
                    from utils.browser import BrowserManager
                    cookie_file = os.path.join(self.storage_path, 'cookies', f'{parser.site_name}.json')
                    if os.path.exists(cookie_file):
                        browser = BrowserManager(headless=True, cookie_file=cookie_file)
                        browser.__enter__()
                        html_content = browser.get_page_content(url)
                    else:
                        print("Ch∆∞a ƒëƒÉng nh·∫≠p. ƒêang t·∫£i b·∫±ng requests...")
                        response = requests.get(url, headers={
                            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                        })
                        html_content = response.text
                except ImportError:
                    print("Selenium ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang t·∫£i b·∫±ng requests...")
                    response = requests.get(url, headers={
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                    })
                    html_content = response.text
            else:
                # S·ª≠ d·ª•ng requests th√¥ng th∆∞·ªùng
                response = requests.get(url, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                })
                html_content = response.text
            
            # Parse n·ªôi dung
            chapter_data = parser.parse_chapter(html_content, url)
            
            return chapter_data
            
        finally:
            if browser:
                browser.__exit__(None, None, None)
    
    def download_story(self, story_url, use_login=False, progress_callback=None):
        """T·∫£i to√†n b·ªô truy·ªán"""
        story_info = self.extract_story_info(story_url)
        if not story_info:
            print("Kh√¥ng th·ªÉ x√°c ƒë·ªãnh th√¥ng tin truy·ªán t·ª´ URL")
            return None
        
        parser = self.get_site_parser(story_url)
        if not parser:
            print("Kh√¥ng h·ªó tr·ª£ site n√†y")
            return None
        
        print(f"Truy·ªán: {story_info['story_name']}")
        print(f"Slug: {story_info['story_slug']}")
        
        # L·∫•y m·ª•c l·ª•c
        toc_url = parser.get_toc_url(story_info)
        print(f"ƒêang l·∫•y m·ª•c l·ª•c t·ª´: {toc_url}")
        
        browser = None
        try:
            # Ki·ªÉm tra ƒëƒÉng nh·∫≠p n·∫øu c·∫ßn
            if use_login and parser.site_config.get('needs_login', False):
                cookie_file = os.path.join(self.storage_path, 'cookies', f'{parser.site_name}.json')
                if not os.path.exists(cookie_file):
                    print("C·∫ßn ƒëƒÉng nh·∫≠p ƒë·ªÉ t·∫£i truy·ªán n√†y")
                    self.login_to_site(parser.site_name, story_info['base_url'])
            
            # L·∫•y HTML m·ª•c l·ª•c
            if use_login and os.path.exists(cookie_file):
                from utils.browser import BrowserManager
                browser = BrowserManager(headless=True, cookie_file=cookie_file)
                browser.__enter__()
                toc_html = browser.get_page_content(toc_url)
            else:
                response = requests.get(toc_url, headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                })
                toc_html = response.text
            
            # Parse m·ª•c l·ª•c
            chapters = parser.parse_toc(toc_html)
            
            # L·ªçc ch·ªâ l·∫•y c√°c chapter h·ª£p l·ªá
            valid_chapters = []
            for ch in chapters:
                if isinstance(ch, str) and ('chuong' in ch.lower() or 'chapter' in ch.lower()):
                    valid_chapters.append(ch)
            
            if not valid_chapters:
                print("Kh√¥ng t√¨m th·∫•y chapter n√†o")
                return None
            
            print(f"T√¨m th·∫•y {len(valid_chapters)} chapter")
            
            # T·∫£i t·ª´ng chapter
            all_chapters = []
            total_chapters = len(valid_chapters)
            
            for i, chapter_url in enumerate(valid_chapters, 1):
                print(f"ƒêang t·∫£i chapter {i}/{total_chapters}")
                
                # T·∫°o URL ƒë·∫ßy ƒë·ªß
                if chapter_url.startswith('http'):
                    full_url = chapter_url
                else:
                    full_url = story_info['base_url'] + chapter_url
                
                # T·∫£i n·ªôi dung chapter
                if use_login and browser:
                    html_content = browser.get_page_content(full_url)
                else:
                    response = requests.get(full_url, headers={
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                    })
                    html_content = response.text
                
                # Parse chapter
                chapter_data = parser.parse_chapter(html_content, full_url)
                all_chapters.append(chapter_data)
                
                # C·∫≠p nh·∫≠t progress
                if progress_callback:
                    progress_callback(i, total_chapters, chapter_data['title'])
            
            # T·∫°o file HTML t·ªïng h·ª£p
            output_file = os.path.join(self.download_path, f"{story_info['story_slug']}.html")
            self.create_html_file(all_chapters, story_info['story_name'], output_file)
            
            # L∆∞u l·ªãch s·ª≠
            self.save_history(story_info, len(all_chapters), output_file)
            
            print(f"ƒê√£ l∆∞u t·∫°i: {output_file}")
            return output_file
            
        except Exception as e:
            print(f"L·ªói khi t·∫£i truy·ªán: {str(e)}")
            import traceback
            traceback.print_exc()
            return None
            
        finally:
            if browser:
                browser.__exit__(None, None, None)
    
    def create_html_file(self, chapters, story_name, output_file):
        """T·∫°o file HTML t·ª´ c√°c chapter"""
        html_content = f"""<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <title>{story_name}</title>
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        body {{ 
            font-family: 'Times New Roman', serif; 
            margin: 40px auto; 
            max-width: 800px; 
            padding: 20px;
            line-height: 1.8;
            background: #f9f9f9;
        }}
        .container {{
            background: white;
            padding: 40px;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
            border-radius: 5px;
        }}
        h1 {{ 
            color: #2c3e50; 
            text-align: center;
            font-size: 28px;
            margin-bottom: 30px;
            padding-bottom: 10px;
            border-bottom: 2px solid #3498db;
        }}
        h2 {{ 
            color: #34495e; 
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 22px;
            padding-left: 10px;
            border-left: 4px solid #3498db;
        }}
        .chapter-content {{ 
            line-height: 1.8;
            font-size: 16px;
            text-align: justify;
        }}
        .chapter-content p {{
            margin-bottom: 15px;
            text-indent: 30px;
        }}
        hr {{ 
            margin: 40px 0; 
            border: none;
            border-top: 1px dashed #bdc3c7;
        }}
        .toc {{
            background: #ecf0f1;
            padding: 20px;
            border-radius: 5px;
            margin-bottom: 30px;
        }}
        .toc ul {{
            list-style: none;
            padding-left: 20px;
        }}
        .toc li {{
            margin-bottom: 8px;
        }}
        .toc a {{
            color: #2980b9;
            text-decoration: none;
            font-size: 15px;
        }}
        .toc a:hover {{
            text-decoration: underline;
            color: #3498db;
        }}
        .chapter {{
            margin-bottom: 50px;
        }}
        @media print {{
            body {{ background: white; }}
            .container {{ box-shadow: none; padding: 0; }}
            .toc {{ display: none; }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>{story_name}</h1>
        
        <div class="toc">
            <h2>üìñ M·ª•c l·ª•c</h2>
            <ul>
"""
        
        # Th√™m m·ª•c l·ª•c
        for i, chapter in enumerate(chapters, 1):
            chapter_title = chapter.get('title', f'Chapter {i}')
            # L√†m s·∫°ch ti√™u ƒë·ªÅ
            chapter_title = chapter_title.replace('"', '&quot;').replace("'", "&#39;")
            html_content += f'        <li><a href="#chapter-{i}">Chapter {i}: {chapter_title}</a></li>\n'
        
        html_content += "    </ul>\n    </div>\n"
        
        # Th√™m n·ªôi dung c√°c chapter
        for i, chapter in enumerate(chapters, 1):
            chapter_title = chapter.get('title', f'Chapter {i}')
            chapter_content = chapter.get('content', '<p>Kh√¥ng c√≥ n·ªôi dung</p>')
            
            # X·ª≠ l√Ω n·ªôi dung
            chapter_content = chapter_content.replace('src="//', 'src="https://')
            
            html_content += f"""
    <div id="chapter-{i}" class="chapter">
        <h2>Chapter {i}: {chapter_title}</h2>
        <div class="chapter-content">
            {chapter_content}
        </div>
        <hr>
    </div>
"""
        
        html_content += """
    </div>
</body>
</html>"""
        
        # L∆∞u file v·ªõi encoding UTF-8
        with open(output_file, 'w', encoding='utf-8', errors='ignore') as f:
            f.write(html_content)
        
        print(f"ƒê√£ t·∫°o file HTML: {output_file}")
    
    def save_history(self, story_info, chapters_count, output_file):
        """L∆∞u l·ªãch s·ª≠ t·∫£i"""
        history_file = os.path.join(self.storage_path, 'history.json')
        
        history = []
        if os.path.exists(history_file):
            with open(history_file, 'r', encoding='utf-8') as f:
                history = json.load(f)
        
        history.append({
            'story_name': story_info['story_name'],
            'story_slug': story_info['story_slug'],
            'chapters': chapters_count,
            'output_file': output_file,
            'timestamp': datetime.now().isoformat(),
            'url': story_info.get('url', '')
        })
        
        # Gi·ªØ 50 b·∫£n ghi g·∫ßn nh·∫•t
        if len(history) > 50:
            history = history[-50:]
        
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(history, f, ensure_ascii=False, indent=2)


class TangThuVienParser:
    """Parser cho trang Tangthuvien.net"""
    
    def __init__(self, site_config, downloader):
        self.site_config = site_config
        self.downloader = downloader
        self.site_name = 'tangthuvien'
    
    def get_toc_url(self, story_info):
        """T·∫°o URL m·ª•c l·ª•c"""
        return f"{story_info['base_url']}/doc-truyen/{story_info['story_slug']}/"
    
    def parse_toc(self, html_content):
        """Parse m·ª•c l·ª•c t·ª´ HTML"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            chapters = []
            
            toc_config = self.site_config['selectors']['toc']
            
            if toc_config['type'] == 'css':
                # Th·ª≠ nhi·ªÅu selector kh√°c nhau
                selectors = toc_config['selector'].split(',')
                for selector in selectors:
                    selector = selector.strip()
                    elements = soup.select(selector)
                    if elements:
                        for element in elements:
                            href = element.get('href')
                            if href:
                                # Chu·∫©n h√≥a URL
                                if not href.startswith('http'):
                                    chapters.append(href)
                                elif self.site_config['domain'] in href:
                                    # L·∫•y path t·ª´ URL ƒë·∫ßy ƒë·ªß
                                    parsed = urlparse(href)
                                    chapters.append(parsed.path)
                        break
            
            # L·ªçc v√† lo·∫°i b·ªè tr√πng l·∫∑p
            unique_chapters = []
            seen = set()
            for ch in chapters:
                if ch not in seen:
                    seen.add(ch)
                    unique_chapters.append(ch)
            
            return unique_chapters
            
        except Exception as e:
            print(f"L·ªói khi parse m·ª•c l·ª•c: {e}")
            return []
    
    def parse_chapter(self, html_content, url):
        """Parse n·ªôi dung chapter"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # L·∫•y ti√™u ƒë·ªÅ
            title = self.extract_title(soup)
            
            # L·∫•y n·ªôi dung
            content = self.extract_content(soup)
            
            # X·ª≠ l√Ω n·ªôi dung
            if content:
                # Th√™m class cho paragraph
                for p in content.find_all('p'):
                    if not p.get('class'):
                        p['class'] = ['chapter-paragraph']
                
                # X·ª≠ l√Ω images
                for img in content.find_all('img'):
                    if img.get('src') and img['src'].startswith('//'):
                        img['src'] = 'https:' + img['src']
                
                content_html = str(content)
            else:
                content_html = '<p>Kh√¥ng th·ªÉ t·∫£i n·ªôi dung chapter</p>'
            
            return {
                'title': title,
                'content': content_html,
                'url': url
            }
            
        except Exception as e:
            print(f"L·ªói khi parse chapter: {e}")
            return {
                'title': 'L·ªói t·∫£i chapter',
                'content': f'<p>C√≥ l·ªói x·∫£y ra khi t·∫£i chapter: {str(e)}</p>',
                'url': url
            }
    
    def extract_title(self, soup):
        """Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ chapter"""
        title_config = self.site_config['selectors']['title']
        
        if title_config['type'] == 'css':
            selectors = title_config['selector'].split(',')
            for selector in selectors:
                selector = selector.strip()
                element = soup.select_one(selector)
                if element:
                    return element.text.strip()
        
        # Fallback: t√¨m trong th·∫ª h1 ho·∫∑c title
        h1 = soup.find('h1')
        if h1:
            return h1.text.strip()
        
        title_tag = soup.find('title')
        if title_tag:
            return title_tag.text.strip()
        
        return "Kh√¥ng c√≥ ti√™u ƒë·ªÅ"
    
    def extract_content(self, soup):
        """Tr√≠ch xu·∫•t n·ªôi dung chapter"""
        content_config = self.site_config['selectors']['content']
        
        if content_config['type'] == 'css':
            selectors = content_config['selector'].split(',')
            for selector in selectors:
                selector = selector.strip()
                element = soup.select_one(selector)
                if element:
                    # X√≥a c√°c th·∫ª kh√¥ng c·∫ßn thi·∫øt
                    for tag in element.find_all(['script', 'style', 'ins', 'iframe']):
                        tag.decompose()
                    
                    # X√≥a c√°c qu·∫£ng c√°o
                    for ad in element.find_all(class_=re.compile(r'ad|ads|banner|quang-cao', re.I)):
                        ad.decompose()
                    
                    return element
        
        return None

def download_story_with_config(self, story_info, site_config, chapters, progress_callback=None):
    """T·∫£i truy·ªán s·ª≠ d·ª•ng c·∫•u h√¨nh site"""
    import requests
    from bs4 import BeautifulSoup
    import time
    
    all_chapters = []
    total = len(chapters)
    
    for i, chapter in enumerate(chapters, 1):
        if not self.downloading:  # Ki·ªÉm tra n·∫øu b·ªã d·ª´ng
            break
            
        try:
            # T·∫°o URL ƒë·∫ßy ƒë·ªß
            if chapter['url'].startswith('http'):
                url = chapter['url']
            else:
                url = story_info['base_url'] + chapter['url']
            
            # Log
            print(f"ƒêang t·∫£i chapter {i}/{total}: {url}")
            
            # L·∫•y HTML
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=30)
            
            if response.status_code == 200:
                # Parse theo c·∫•u h√¨nh
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # L·∫•y title
                title_config = site_config['selectors']['title']
                if title_config['type'] == 'css':
                    title_elem = soup.select_one(title_config['script'])
                    title = title_elem.text.strip() if title_elem else f"Chapter {i}"
                else:
                    title = f"Chapter {i}"
                
                # L·∫•y content
                content_config = site_config['selectors']['content']
                if content_config['type'] == 'css':
                    content_elem = soup.select_one(content_config['script'])
                    if content_elem:
                        # X√≥a script, style
                        for tag in content_elem.find_all(['script', 'style', 'ins']):
                            tag.decompose()
                        content = str(content_elem)
                    else:
                        content = "<p>Kh√¥ng th·ªÉ t·∫£i n·ªôi dung</p>"
                else:
                    content = "<p>Kh√¥ng th·ªÉ t·∫£i n·ªôi dung</p>"
                
                all_chapters.append({
                    'title': title,
                    'content': content,
                    'url': url
                })
                
                # Callback progress
                if progress_callback:
                    progress_callback(i, total, title)
                
                time.sleep(1)  # Tr√°nh spam request
            else:
                print(f"L·ªói HTTP {response.status_code} khi t·∫£i {url}")
                
        except Exception as e:
            print(f"L·ªói khi t·∫£i chapter {i}: {str(e)}")
            all_chapters.append({
                'title': f"Chapter {i} (L·ªói)",
                'content': f"<p>C√≥ l·ªói x·∫£y ra khi t·∫£i chapter: {str(e)}</p>",
                'url': chapter['url']
            })
    
    return all_chapters

def main():
    """H√†m ch√≠nh ƒë·ªÉ ch·∫°y t·ª´ command line"""
    import argparse
    
    parser = argparse.ArgumentParser(description='T·∫£i truy·ªán t·ª´ c√°c trang web')
    parser.add_argument('url', help='URL c·ªßa truy·ªán c·∫ßn t·∫£i')
    parser.add_argument('--login', action='store_true', help='S·ª≠ d·ª•ng ƒëƒÉng nh·∫≠p ƒë·ªÉ t·∫£i')
    parser.add_argument('--config', default='config.json', help='File c·∫•u h√¨nh')
    
    args = parser.parse_args()
    
    downloader = TruyenDownloader(args.config)
    
    print("=" * 60)
    print("TRUY·ªÜN DOWNLOADER")
    print("=" * 60)
    
    if 'doc-truyen' in args.url:
        result = downloader.download_story(args.url, args.login)
        if result:
            print(f"\n‚úÖ T·∫£i th√†nh c√¥ng! File: {result}")
        else:
            print("\n‚ùå T·∫£i th·∫•t b·∫°i!")
    else:
        print("URL kh√¥ng h·ª£p l·ªá. Vui l√≤ng nh·∫≠p URL trang truy·ªán.")

if __name__ == '__main__':
    main()